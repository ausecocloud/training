{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biomass Estimation - Putting the RAT into TPOT  \n",
    "<img src='http://www.auscover.org.au/wp-content/themes/auscover/assets/images/logo.png'>\n",
    "\n",
    "\n",
    "#### A demonstration on linking field data to image data using machine learning\n",
    "_DISCLAIMER: This is only a demonstration of some cool data science methods you can pull off on your laptop at home, and not about the science of Biomass estimation. There are some very good scientists and programs looking at how to improve biomass estimation and better quantify the error budget. Whatever comes out of the bottom of this worksheet is to be used for your amusement only :)_\n",
    "\n",
    "\n",
    "_By [Peter Scarth](mailto:p.scarth@uq.edu.au?subject=AEOCCG%20webinar%20information) (Joint Remote Sensing Research Program) _\n",
    "\n",
    "### Abstract\n",
    "\n",
    "This is a quick and dirty notebook demonstrating how to link two data sets I pulled off the [AusCover](http://auscover.org.au) portal. In particular, I wanted to show how powerful the combination of Raster Attribute Tables (RATs) and Machine Learning (ML) is for getting quick insights into data at a national scale.  I pulled this together one Sunday night so it's not necessarily pretty or efficient but it might give someone else a heads up into getting started with integrated ecological data science using [TERN](http://www.tern.org.au/) data.\n",
    "\n",
    "This uses a number of awesome python libraries. I'd really recommend [conda](https://conda.io/) to get going, or if you like Docker pull my [datascience-rs](https://hub.docker.com/r/petescarth/datascience-rs/) container that has everything you need to run this installed.\n",
    "\n",
    "\n",
    "#### Raster Attribute Tables\n",
    "For an excellent introduction to RATs I recommend the following paper:\n",
    "\n",
    "Clewley, D.; Bunting, P.; Shepherd, J.; Gillingham, S.; Flood, N.; Dymond, J.; Lucas, R.; Armston, J.; Moghaddam, M. _A Python-Based Open Source System for Geographic Object-Based Image Analysis (GEOBIA) Utilizing Raster Attribute Tables_. [__Remote Sensing 2014, 6, 6111-6135__](http://www.mdpi.com/2072-4292/6/7/6111). \n",
    "\n",
    "RATs work well with machine learning and quick data analysis. To run the prediction across the continent using this notebook takes about __30 minutes__ (depends on the final model) when working on the RAT. It would take around 6000x longer or nearly __18 weeks__ to process the image on a per-pixel basis.\n",
    "\n",
    "If you want a lightweight raster GIS with powerful RAT manipulation abilities have a look at [Tuiview](http://tuiview.org/)\n",
    "\n",
    "#### Machine Learning\n",
    "I really like [TPOT](http://rhiever.github.io/tpot/) for this sort of analysis. It's close to magical for me. You can possibly do better through careful ML pipeline selection and tuning but it really helps to get you started.\n",
    "_\"Consider TPOT your Data Science Assistant. TPOT is a Python tool that automatically creates and optimizes machine learning pipelines using genetic programming.\"_\n",
    "\n",
    "#### Biomass Library\n",
    "The [Biomass Plot Library](http://www.auscover.org.au/purl/biomass-plot-library) is  a national collation of tree and shrub inventory data, allometric model predictions of above and below-ground biomass for Australia. More data is being added all the time.\n",
    "\n",
    "The data in this notebook was extracted using the [Field Data Portal](http://qld.auscover.org.au/public/html/field/). I selected the __Biomass__ Library and then the __Site Level__ Dataset.  I downloaded the whole lot as a csv file and that file (`biolib_sitelist_auscover.csv`) is what I use here.\n",
    "\n",
    "<img src='http://www.auscover.org.au/wp-content/uploads/biomass-plot-library.jpg' >\n",
    "\n",
    "#### Height and Structure Data\n",
    "The [Vegetation height and structure - derived from ALOS-1 PALSAR, Landsat and ICESat/GLAS, Australia](http://auscover.org.au/purl/icesat-vegetation-structure  ) contains vertical plant profiles for the Australian continent derived through integration of ICESat GLAS waveforms with ALOS PALSAR and Landsat data products. \n",
    "\n",
    "There are individual tiff files of the various parameters available for download, but you're going to want the  [KEA](http://www.gdal.org/frmt_kea.html) file of all the parameters and the segments (`alpsbk_aust_y2009_sc5a2.kea`) which is available [here](http://qld.auscover.org.au/public/data/spatial_other/jrsrp/height/alpsbk_aust_y2009_sc5a2.kea) as a __9.1 G__ download.  I'm yet to work out how to efficiently access kea files over the internet, like we do using the /vsicurl/ driver for GeoTIFF, so for now you'll need to download it.\n",
    "\n",
    "\n",
    "<img src='http://www.auscover.org.au/wp-content/uploads/Vegetation-height-and-structure.jpg' >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 'rios' already in 'channels' list, moving to the top\n"
     ]
    }
   ],
   "source": [
    "!conda config --append channels rios\n",
    "!conda config --append channels conda-forge\n",
    "!conda install -y -n py35 rios tpot\n",
    "\n",
    "# conda install -y -n py35 rasterstats netCDF4 pylidar python-fmask rsgislib datacube arcsi pysolar py6s sixs=1.1 h5py numpy scipy sympy numba matplotlib xerces-c geopandas folium geoplot siphon astropy plotly psycopg2 sshtunnel ipyparallel xarray dask jsonschema pyyaml sqlalchemy cachetools cartopy holoviews postgresql bottleneck python-dateutil pathlib click distributed pyparsing keras tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for this Python3 notebook\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from osgeo import gdal\n",
    "from osgeo import ogr\n",
    "from osgeo import osr\n",
    "\n",
    "from rios import rat\n",
    "from rios import ratapplier\n",
    "\n",
    "from tpot import TPOTRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training data\n",
    "This section reads in the Biomass library csv file and links the observations to the closest segment.  \n",
    "NOTE: _No consideration is made of the size or shape of the field plot or the shapes of the segments!_\n",
    "\n",
    "The Biomass Library has the following fields:\n",
    "`longitude,latitude,live_basal_area_ha,dead_basal_area_ha,live_tree_density_ha,dead_tree_density_ha,\n",
    "agb_drymass_ha,agb_drymass_se_ha,bgb_drymass_ha,bgb_drymass_se_ha,tb_drymass_ha,tb_drymass_se_ha`  \n",
    "\n",
    "Once we find the closest segment, we extract the attributes we want from the RAT to aget an _imageData_ array to match with the _fieldBiomass_ array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 20.9M    0 20.9M    0     0  2463k      0 --:--:--  0:00:08 --:--:-- 1786k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 9290M  100 9290M    0     0   115M      0  0:01:20  0:01:20 --:--:--  118M\n"
     ]
    }
   ],
   "source": [
    "!curl -Lo biolib_sitelist_auscover.csv 'http://qld.auscover.org.au/geoserver/wfs?service=WFS&version=1.0.0&request=GetFeature&typeName=aus%3Abiolib_sitelist_auscover&outputFormat=csv&srs=EPSG%3A4326&cql_filter=(obs_time%20BETWEEN%201980-01-01%20AND%202018-06-19)%20AND%20INTERSECTS(geom%2CMULTIPOLYGON%20(((130.341796875%20-9.79567758282973%2C%20111.884765625%20-21.861498734372553%2C%20114.43359375%20-36.38591277287651%2C%20131.220703125%20-32.990235559651055%2C%20142.998046875%20-39.97712009843962%2C%20145.107421875%20-43.96119063892025%2C%20149.4140625%20-44.46515101351962%2C%20154.51171875%20-31.05293398570514%2C%20153.80859375%20-23.24134610238612%2C%20146.865234375%20-16.383391123608387%2C%20142.20703125%20-7.536764322084078%2C%20139.5703125%20-14.093957177836224%2C%20137.197265625%20-9.709057068618208%2C%20134.6484375%20-10.487811882056695%2C%20130.341796875%20-9.79567758282973))))'\n",
    "\n",
    "!curl -LO 'http://qld.auscover.org.au/public/data/spatial_other/jrsrp/height/alpsbk_aust_y2009_sc5a2.kea'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "fieldBiomass = pandas.read_csv(\"biolib_sitelist_auscover.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linking field observations to segment IDs\n",
      "\n",
      "Extracting sites from PGmean\n",
      "Extracting sites from PGsd\n",
      "Extracting sites from HHmean\n",
      "Extracting sites from HHsd\n",
      "Extracting sites from HVmean\n",
      "Extracting sites from HVsd\n",
      "Extracting sites from rhogc1000\n",
      "Extracting sites from fcovc1000\n",
      "Extracting sites from rhovc1000\n",
      "Extracting sites from gSDc1000\n",
      "Extracting sites from fcov_10_30c1000\n",
      "Extracting sites from fcov_totalc1000\n",
      "Extracting sites from fcov_30plusc1000\n",
      "Extracting sites from fcov_0_5c1000\n",
      "Extracting sites from fcov_5_10c1000\n",
      "Extracting sites from hp50c1000satcor\n",
      "Extracting sites from hp75c1000satcor\n",
      "Extracting sites from hp95c1000satcor\n",
      "Extracting sites from hp25c1000satcor\n",
      "Extracting sites from hModec1000satcor\n",
      "\n",
      "Training data has 14439 observations and 20 columns\n"
     ]
    }
   ],
   "source": [
    "# Read Biomass library data from the csv file \n",
    "# fieldBiomass=numpy.loadtxt(\"biolib_sitelist_auscover.csv\",delimiter=',', skiprows=1)\n",
    "\n",
    "# Open Height Map dataset\n",
    "keaFile = \"alpsbk_aust_y2009_sc5a2.kea\"\n",
    "\n",
    "heightDataset = gdal.Open(keaFile, gdal.GA_Update)\n",
    "\n",
    "\n",
    "# Set up the reprojection transform from WGS84 (biomass library) to Australian Albers (height data)\n",
    "source = osr.SpatialReference()\n",
    "source.ImportFromEPSG(4326)\n",
    "target = osr.SpatialReference()\n",
    "target.ImportFromEPSG(3577)\n",
    "transform = osr.CoordinateTransformation(source, target)\n",
    "\n",
    "\n",
    "# Open the raster band with the segment IDs\n",
    "heightBand=heightDataset.GetRasterBand(1)\n",
    "\n",
    "# Get the Albers to pixel transform\n",
    "geoTransform=heightDataset.GetGeoTransform()\n",
    "\n",
    "# Find the segmentID for all the field sites\n",
    "print(\"Linking field observations to segment IDs\\n\")\n",
    "segmentIDs = []\n",
    "for idx, record in fieldBiomass.iterrows():\n",
    "    # Make up a site OGR point\n",
    "    site = ogr.Geometry(ogr.wkbPoint)\n",
    "    site.AddPoint(record.longitude, record.latitude)\n",
    "    # Transform the site to EPSG3577\n",
    "    site.Transform(transform)\n",
    "    # Get the pixel location of the site\n",
    "    mx,my=site.GetX(), site.GetY()  #coord in map units\n",
    "    #Convert from map to pixel coordinates.\n",
    "    #Only works for geotransforms with no rotation.\n",
    "    px = int((mx - geoTransform[0]) / geoTransform[1]) #x pixel\n",
    "    py = int((my - geoTransform[3]) / geoTransform[5]) #y pixel\n",
    "    # Extract the segmentID for the location\n",
    "    segmentIDs.append(heightBand.ReadAsArray(px,py,1,1)[0][0])\n",
    "\n",
    "# Get the RAT column names\n",
    "colNames = rat.getColumnNames(heightDataset)\n",
    "\n",
    "# Select the columns used for the training/prediction\n",
    "trainingColumns = [5,6,7,8,9,10,15,19,22,23,25,26,27,28,29,30,31,32,33,34]\n",
    "trainingNames = [colNames[i] for i in trainingColumns]\n",
    "\n",
    "# Now we have the segmentIDs, pull the image data from the RAT that corresponds to the segment IDs\n",
    "imageData = []\n",
    "# Iterate for all the RAT columns\n",
    "for name in trainingNames:\n",
    "    print(\"Extracting sites from \" + name)\n",
    "    # Extract the array of values corresponding to the field site segments\n",
    "    imageData.append(rat.readColumnFromBand(heightBand,name).astype('float')[segmentIDs])\n",
    "\n",
    "# Convert the list of arrays to an array\n",
    "imageData = numpy.transpose(numpy.array(imageData))\n",
    "\n",
    "# Remove nodata from the couple of segments too small to get statistics in the image data\n",
    "goodDataIDX = imageData.min(axis=1)>0\n",
    "imageData = imageData[goodDataIDX]\n",
    "fieldBiomass = fieldBiomass[goodDataIDX]\n",
    "\n",
    "\n",
    "print(\"\\nTraining data has %d observations and %d columns\" % imageData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Machine Learning Model\n",
    "This uses the amazing [TPOT](https://github.com/rhiever/tpot) tool to find a good machine learning pipeline that predicts biomass. It automates the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one for your data. There are a few setting below to tweak.  \n",
    "  - _optTime_ is the number of minutes you are prepared to wait for the optimisation. The longer the better.\n",
    "  - _nCPUs_ is the number of CPUs you have available for the search. You'll also need a minimum of about 2GB of RAM per CPU\n",
    "  - _Scoring_ is the thing you'll be minimising\n",
    "  - _nSubsets_ picks a random subset from the training data. Smaller is faster but you might miss some variation in the data\n",
    "  - The field from the biomass library you want to fit. Here we're selecting the __Total Biomass__ column (number 10)\n",
    "\n",
    "This will run a genetic algorithm until it times out and selects the best fitting model to use for biomass prediction. It saves the model so you can play with it later if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBRegressor is not available and will not be used by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress: 253pipeline [1:03:00, 16.90s/pipeline]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: -287611.4592310187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress: 359pipeline [1:12:01,  4.21s/pipeline]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 2 - Current best internal CV score: -286718.30674723105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress: 463pipeline [1:22:49,  5.83s/pipeline]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 3 - Current best internal CV score: -286233.7665528875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress: 572pipeline [1:37:19,  6.80s/pipeline]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 4 - Current best internal CV score: -286232.4852288223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress: 679pipeline [1:53:08,  8.82s/pipeline]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 5 - Current best internal CV score: -286232.4852288223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress: 792pipeline [2:09:58,  9.15s/pipeline]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 6 - Current best internal CV score: -286232.4852288223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress: 906pipeline [2:31:10,  9.55s/pipeline]                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 7 - Current best internal CV score: -286217.36785688373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress: 1019pipeline [2:49:46,  8.83s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 8 - Current best internal CV score: -285795.48402256414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress: 1129pipeline [3:07:51,  9.40s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 9 - Current best internal CV score: -285792.97804794065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress: 1246pipeline [3:27:03,  9.71s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 10 - Current best internal CV score: -285792.97804794065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress: 1357pipeline [3:46:30, 11.54s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 11 - Current best internal CV score: -285612.4791654758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress: 1466pipeline [4:07:02, 10.29s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 12 - Current best internal CV score: -285176.28355825314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress: 1578pipeline [4:26:39, 10.68s/pipeline]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 13 - Current best internal CV score: -285173.8225255059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization Progress: 1676pipeline [4:44:57,  9.86s/pipeline]"
     ]
    }
   ],
   "source": [
    "# Total amount of time we allow for the training\n",
    "optTime = 600\n",
    "\n",
    "# Number of CPUs to use for training and cross validation\n",
    "nCPUs = 4\n",
    "\n",
    "# What function to minimise\n",
    "scoring = 'mean_squared_error'\n",
    "\n",
    "# Number of subsamples from the Biomass Library for model training\n",
    "nSubsets = 9999\n",
    "\n",
    "# We select the tb_drymass_ha column to train on\n",
    "# totalBiomass = fieldBiomass[:,10]\n",
    "totalBiomass = fieldBiomass['tb_drymass_ha'].values\n",
    "totalBiomass = totalBiomass[~numpy.isnan(totalBiomass)]\n",
    "# This is the standard error of the site level estimates\n",
    "totalBiomassSE = fieldBiomass['tb_drymass_se_ha'].values\n",
    "totalBiomassSE = totalBiomassSE[~numpy.isnan(totalBiomassSE)]\n",
    "\n",
    "# Select a subsample to improve the model search speed\n",
    "subSample = numpy.random.choice(len(totalBiomass),nSubsets,replace=False)\n",
    "biomass=totalBiomass[subSample]\n",
    "biomassSE=totalBiomassSE[subSample]\n",
    "trainData=imageData[subSample]\n",
    "\n",
    "# Use the proportion of the error in the estimates as fitting weights\n",
    "biomassWeights=biomass/biomassSE\n",
    "\n",
    "# Setup the TPOT regression options\n",
    "tpot = TPOTRegressor(max_time_mins=optTime,  \n",
    "                     n_jobs = nCPUs, \n",
    "                     scoring=scoring, \n",
    "                     verbosity=2, \n",
    "                     cv=10, \n",
    "                     max_eval_time_mins=1,\n",
    "                     population_size=100)\n",
    "\n",
    "# Start testing models using 10 fold cross validation and 100 models per generation\n",
    "tpot.fit(trainData, biomass, sample_weight=biomassWeights)\n",
    "\n",
    "# Export the best model to a file\n",
    "tpot.export('tpot_biomass_pipeline.py')\n",
    "\n",
    "# Build the biomass predictive model\n",
    "biomassModel = tpot._fitted_pipeline.fit(imageData, totalBiomass)\n",
    "\n",
    "# Predict the full dataset\n",
    "predBiomass = biomassModel.predict(imageData)\n",
    "\n",
    "# Print some RMSE Statistics for various ranges\n",
    "print(\"\\nTotal RMSE = %f\\n\" % numpy.sqrt(numpy.mean((totalBiomass-predBiomass)**2)))\n",
    "stopPoints=[0,100,500,1000,2000,5000,10000]\n",
    "print(\"Start\",\" Stop\",\"Count\",\" RMSE\")\n",
    "for i in range(len(stopPoints)-1):\n",
    "    idx=numpy.logical_and(totalBiomass>stopPoints[i],totalBiomass<stopPoints[i+1])\n",
    "    rmse=numpy.sqrt(numpy.mean((totalBiomass[idx]-predBiomass[idx])**2))\n",
    "    print('{0:5d} {1:5d} {2:5d} {3:5.0f}'.format(stopPoints[i],stopPoints[i+1],idx.sum(),rmse))\n",
    "\n",
    "\n",
    "\n",
    "# Plot the Output in a LogLog figure\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.loglog(totalBiomass,predBiomass, 'g.',[10,10000], [10,10000],'r-')\n",
    "plt.xlabel('Observed (Mg/ha)', fontsize=18)\n",
    "plt.ylabel('Predicted (Mg/ha)', fontsize=18)\n",
    "plt.title('Total Biomass Estimate', fontsize=32)\n",
    "plt.xlim([10,10000])\n",
    "plt.ylim([10,10000])\n",
    "plt.grid(which='minor', alpha=0.4)                                                \n",
    "plt.grid(which='major', alpha=0.8)                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Biomass using the RAT\n",
    "Once we have the _biomassModel_ we can use that to predict the Biomass for every entry in the RAT. \n",
    "\n",
    "This is cool, because for Australia we only need to make around 3,000,000 predictions which is far faster than predicting every one of the 18,407,102,543 pixels in the image. That's around a __6000x__ speedup.\n",
    "\n",
    "Here,  [RIOS](http://rioshome.org) ratapplier does the work writing a brand new attribute column into the kea file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def _ratapplier_calc_biomass(info, inputs, outputs):\n",
    "    \"\"\"\n",
    "    Calculate Biomass from RAT.\n",
    "    Called by ratapplier below\n",
    "    \"\"\"\n",
    "    ratArray = []\n",
    "    # Iterate for all the RAT columns\n",
    "    for name in trainingNames:\n",
    "        # Extract the array of values corresponding to the field site segments\n",
    "        ratArray.append(getattr(inputs.inrat, name).astype('float'))\n",
    "\n",
    "    # Convert the list of arrays to an array\n",
    "    ratArray = numpy.transpose(numpy.array(ratArray))\n",
    "    # Predict Biomass\n",
    "    biomass = biomassModel.predict(ratArray)\n",
    "    # Make the weird inputs nodata\n",
    "    biomass[ratArray.min(axis=1) < numpy.finfo(numpy.float32).eps] = 0\n",
    "\n",
    "    # Save to 'totalBiomass' column (will create if doesn't exist)\n",
    "    setattr(outputs.outrat,\"totalBiomass\", biomass)\n",
    "\n",
    "\n",
    "# Set up ratapplier for input / output\n",
    "inFile = ratapplier.RatAssociations()\n",
    "outFile = ratapplier.RatAssociations()\n",
    "\n",
    "# Pass in clumps file for the input and output as we'll be updating the existing RAT\n",
    "inFile.inrat = ratapplier.RatHandle(keaFile)\n",
    "outFile.outrat = ratapplier.RatHandle(keaFile)\n",
    "\n",
    "# Apply function to all rows in chunks\n",
    "ratapplier.apply(_ratapplier_calc_biomass, inFile, outFile)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Biomass band to an individual GeoTIFF\n",
    "You can run this section if you want to output just the newly calculated biomass column to a separate GeoTIFF. \n",
    "\n",
    "This uses one of the function from the rastergis module in [RSGISLIB](http://www.rsgislib.org/)\n",
    "\n",
    "After this, you might like to run __gdal_translate__ to compress the output and __gdaladdo__ to add overviews for easy storage and fast viewing in your favorite GIS platform. Like this:\n",
    "```\n",
    "gdal_translate -a_nodata 0 -of GTIFF -co COMPRESS=DEFLATE -co ZLEVEL=9 -co TILED=YES totalBiomass.tif totalBiomassCompressed.tif\n",
    "\n",
    "gdaladdo -r average --config TILED_OVERVIEW YES --config COMPRESS_OVERVIEW PACKBITS totalBiomassCompressed.tif 4 8 16 32 64 128 256 512\n",
    "\n",
    "```\n",
    "\n",
    "If everything has worked, you should have a file that looks something like this:\n",
    "\n",
    "<img src='https://pbs.twimg.com/media/C-s2iyTUwAAxNzb.jpg' >\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Additional Imports\n",
    "import rsgislib\n",
    "from rsgislib.rastergis import exportCol2GDALImage\n",
    "\n",
    "# Setup the export parameters\n",
    "outimage='/home/jovyan/work/Temp/totalBiomass.tif'\n",
    "gdalformat = 'GTIFF'\n",
    "datatype = rsgislib.TYPE_16UINT\n",
    "field = 'totalBiomass'\n",
    "\n",
    "# Run the export\n",
    "exportCol2GDALImage(keaFile, outimage, gdalformat, datatype, field)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py35)",
   "language": "python",
   "name": "conda_py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
